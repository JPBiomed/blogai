{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jpbiomed.github.io/blogai/jupyter/2021/06/28/Hidden-test.html",
            "relUrl": "/jupyter/2021/06/28/Hidden-test.html",
            "date": " • Jun 28, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Apprendre avec Fast AI",
            "content": "Cr&#233;er son premier mod&#232;le . Commençons par choisir un sujet pour mon premier projet. . Voilà mes idées: . Classification de pelouse arrosée ou sèche | Créateur de mail pour prendre des nouvelles d&#39;amis | Classification d&#39;images de malaria infectées ou non infectées | . Avant de lancer mon premier modèle, il faut que j&#39;ai des données. . Obtenir des donn&#233;es . On peut trouver des banques de données en ligne. . Voilà les sources de données auxquelles j&#39;ai pensé: . Pelouse : Je vais obtenir les images sur base de l&#39;API de Bing Image (c&#39;est pareil que Google Image) | Mail : Mes derniers messages à des amis et en écrire une dizaine en plus | Malaria : J&#39;ai trouvé des datasets sur Kaggle, une plate-forme de compétition de Data Science, allez voir c&#39;est super intéressant ! | . Pour ce premier projet, je commence par les pelouses ! . key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;XXX&#39;) . results = search_images_bing(key, &#39;dry lawns&#39;) ims = results.attrgot(&#39;content_url&#39;) len(ims) . ErrorResponseException Traceback (most recent call last) &lt;ipython-input-3-cddb73f3292e&gt; in &lt;module&gt; -&gt; 1 results = search_images_bing(key, &#39;grizzly bear&#39;) 2 ims = results.attrgot(&#39;content_url&#39;) 3 len(ims) /opt/conda/envs/fastai/lib/python3.8/site-packages/fastbook/__init__.py in search_images_bing(key, term, min_sz) 50 def search_images_bing(key, term, min_sz=128): 51 client = api(&#39;https://api.cognitive.microsoft.com&#39;, auth(key)) &gt; 52 return L(client.images.search(query=term, count=150, min_height=min_sz, min_width=min_sz).value) 53 54 def plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)): /opt/conda/envs/fastai/lib/python3.8/site-packages/azure/cognitiveservices/search/imagesearch/operations/_images_operations.py in search(self, query, accept_language, user_agent, client_id, client_ip, location, aspect, color, country_code, count, freshness, height, id, image_content, image_type, license, market, max_file_size, max_height, max_width, min_file_size, min_height, min_width, offset, safe_search, size, set_lang, width, custom_headers, raw, **operation_config) 489 490 if response.status_code not in [200]: --&gt; 491 raise models.ErrorResponseException(self._deserialize, response) 492 493 deserialized = None ErrorResponseException: Operation returned an invalid status code &#39;PermissionDenied&#39; . dest = &#39;images/dry_lawn.jpg&#39; download_url(ims[0], dest) . bear_types = &#39;dry&#39;,&#39;&#39; path = Path(&#39;lawns&#39;) . if not path.exists(): path.mkdir() for o in bear_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o} lawns&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . fns = get_image_files(path) fns . failed = verify_images(fns) failed . failed.map(Path.unlink); . Charger les images pour le mod&#232;le . Un modèle de deep learning a des millions de paramètres comme nous l&#39;avons dit. Donc pour les ajuster tous, cela prend beaucoup de temps. Pour faire cela plus rapidement, on fait l&#39;optimisation en parallèle. Pour ce faire, on utilise un GPU (Graphical Processing Unit) qui est spécialisé dans les calculs en parallèle et on envoie les données par &quot;batch&quot; ou par lot en francais. . Par défaut, les lots de fast.ai sont de 64 images. . Avant d&#39;envoyer ces données, il est nécessaire de les préparer et puis de les rassembler en lot qu&#39;on enverra au GPU. . Pour les préparer, fast.ai fournit les DataBlock et demandent quelques informations concernant le modèle: . blocks : le type de données : dans notre cas, c&#39;est des images d&#39;une part et des catégories d&#39;autre part, (pelouses sèches vs pelouses humides) | get_items: le chemin d&#39;accès des images : ici c&#39;est grâce à la fonction get_image_files | splitter : la division entre ensemble d&#39;entraînement et de validation | get_y : la définition des catégories ou de l&#39;ensemble «cible», typiquement appelé y. Ici c&#39;est le type de pelouse. Ici parent_label reprendra directement le nom du dossier parent. | item_tfms : la méthode de transformation des images : En effet, pour que l&#39;algorithme puisse travailler, il est nécessaire que toutes les images aient la même taille. Classiquement, on choisit de prendre une image carrée. On l&#39;obtenir, on a plusieurs options, qu&#39;on définira avec l&#39;attribut item_tfms ici on choisit Resize qui va récupérer un carré au centre de l&#39;image. | . bears = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . La création d&#39;un DataBlock permet de créer des DataLoaders quand on fournit le dossier cible. . dls = bears.dataloaders(path) . Maintenant qu&#39;on a chargé les images, il faut encore vérifier qu&#39;elles sont toujours pertinentes et qu&#39;elles n&#39;ont pas été rogné trop fort. . On peut faire cela avec la méthode valid.show_batch . dls.valid.show_batch(max_n=4, nrows=1) . Exporter son premier mod&#232;le . gv(&#39;&#39;&#39;&quot;Architecture et Methode d&#39;Apprentissage&quot;[shape=box3d width=1 height=0.7] Données-&gt;&quot;Architecture et Methode d&#39;Apprentissage&quot;-&gt; Capacité&#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G Architecture et Methode d&#39;Apprentissage Architecture et Methode d&#39;Apprentissage Capacité Capacité Architecture et Methode d&#39;Apprentissage&#45;&gt;Capacité Données Données Données&#45;&gt;Architecture et Methode d&#39;Apprentissage",
            "url": "https://jpbiomed.github.io/blogai/2021/06/24/FastAi.html",
            "relUrl": "/2021/06/24/FastAi.html",
            "date": " • Jun 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Apprendre avec Fast AI",
            "content": "Les bases . Le probl&#232;me : apprendre &#224; un ordinateur . Le deeplearning permet qu&#39;un ordinateur apprenne tout seul à analyser des données. Cela peut donc permettre d&#39;apprendre à un ordinateur de reconnaître un chat alors que un humain aurait énormément de difficultés à lui expliquer comment le reconnaître. . Par exemple, si je vous demande de décrire un chat comme celui-ci, que diriez-vous ? Image d&#39;un chat . On pourrait dire que : . il a des pattes | il a des poils | il a des oreilles qui dépassent de sa tête | il a une queue | ... | . Cette définition fait déjà appel à d&#39;autres concepts (pattes, poils, oreilles, tête, queue, ...) qu&#39;ils faudraient également expliquer à l&#39;ordinateur mais en plus de cela, cette définition pourrait très bien s&#39;appliquer à cette image-ci : Image d&#39;un chien . Au final, il se trouve que tout expliquer à un ordinateur est impossible et que donc il est plus pratique qu&#39;il apprenne tout seul. . Le deeplearning : de la m&#233;moire et une m&#233;thode . Faire quelque chose, on l&#39;appelera une capacité. Une capacité sera donc par exemple de reconnaître un chat. . Pour apprendre, un ordinateur a besoin d&#39;une méthode d&#39;apprentissage qu&#39;on verra au fur et à mesure. C&#39;est l&#39;architecture du réseau de neurones et la méthode de modification des paramètres, ensemble c&#39;est un modèle. Chez nous les humains, on l&#39;appelle le cerveau. L&#39;architecture c&#39;est notre anatomie et les connexions entre nos neurones. Chacun en a un différent et c&#39;est pour ça que certains apprennent plus vite que d&#39;autres, retiennent plus longtemps. Mais chacun a des compétences différentes, certains seront fort en maths, d&#39;autres en sports, en musique ou comprendront mieux les émotions des autres. . De la même manière, certaines architectures sont plus appropriées pour certaines tâches que pour d&#39;autres. . Pour retenir ce qu&#39;il apprend, il utilisera des milliers ou millions de paramètres, qui lui permettront d&#39;analyser ce qu&#39;il voit à chaque étape. L&#39;ordinateur modifie les paramètres pour se rapprocher de la bonne réponse. . Chaque paramètre est un élément du modèle et trouve sa place dans l&#39;architecture utilisée. Un ensemble de paramètres pourra être optimal pour une application spécifique (ex: différencier les chats et les chiens, détecter la présence ou l&#39;absence de caries sur un radio dentaire, ...) mais ne fonctionnera pas comme cela pour une autre application. Par contre, une architecture peut être utilisée pour plusieurs applications différentes mais devra être adaptées . Et pour finir, il faut du matériel à étudier : les données. . Les données, c&#39;est un peu le livre d&#39;étude de l&#39;ordinateur, c&#39;est là cela sa source d&#39;informations alors évidemment plus il en a plus il a de grandes capacités . Mais l&#39;important c&#39;est que ces données soient annotées, en effet, l&#39;ordinateur doit savoir ce qu&#39;il cherche . gv(&#39;&#39;&#39;&quot;Architecture et Methode d&#39;Apprentissage&quot;[shape=box3d width=1 height=0.7] Données-&gt;&quot;Architecture et Methode d&#39;Apprentissage&quot;-&gt; Capacité&#39;&#39;&#39;) . NameError Traceback (most recent call last) &lt;ipython-input-8-92956a06600f&gt; in &lt;module&gt; -&gt; 1 gv(&#39;&#39;&#39;&#34;Architecture et Methode d&#39;Apprentissage&#34;[shape=box3d width=1 height=0.7] 2 Données-&gt;&#34;Architecture et Methode d&#39;Apprentissage&#34;-&gt; Capacité&#39;&#39;&#39;) ~/anaconda3/lib/python3.8/site-packages/fastbook/__init__.py in gv(s) 39 if IN_COLAB: return setup_colab() 40 &gt; 41 def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&#34;LR&#34;&#39; + s + &#39;; }&#39;) 42 43 def get_image_files_sorted(path, recurse=True, folders=None): NameError: name &#39;graphviz&#39; is not defined . gv(&#39;&#39;&#39;&quot;Cerveau humain&quot;[shape=box3d width=1 height=0.7] &quot;Livre de référence&quot;-&gt;&quot;Cerveau humain&quot;-&gt;Capacité&#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G Cerveau humain Cerveau humain Capacité Capacité Cerveau humain&#45;&gt;Capacité Livre de référence Livre de référence Livre de référence&#45;&gt;Cerveau humain gv(&#39;&#39;&#39;&quot;Cerveau humain&quot;[shape=box3d width=1 height=0.7] &quot;Livre de référence&quot;-&gt;&quot;Cerveau humain&quot;-&gt;Capacité&#39;&#39;&#39;) . Comment &#231;a marche ? . Je vais résumer les choses. Si vous voulez comprendre dans le détail, je vous recommande d&#39;aller suivre le cours directement (https://course.fast.ai). . En gros, le deep learning se base sur le fait qu&#39;avec beaucoup de petits éléments très simples mais avec beaucoup d&#39;interconnexion, il y a moyen de tout apprendre. . On appelle ces éléments des neurones. Il sont tous simples parce que pour la plupart, il font une somme pondérée de tous les neurones auxquels ils sont connectés. . Une somme pondérée c&#39;est par exemple, un ticket de caisse au supermarché, on additione le nombre d&#39;articles multipliés par le prix de chaque article. . Ensuite, si on modifie ces paramètres, on pourra apprendre ce que l&#39;on veut. . Pour augmenter le niveau d&#39;abstraction, c&#39;est-à-dire permettre à l&#39;ordinateur d&#39;apprendre des choses de plus en plus complexes, on augmentera le nombre de niveau. . Un réseau de neurones de convolution peut être vu comme ceci. Convolutionnal Neural Network . À chaque niveau, on détecte des caractéristiques plus complexes: . Aux premiers niveaux, elles sont très simples, lignes obliques à gauche, couleur unie, ligne horizontale, ... | Aux niveaux intermédiaires, on voit apparaitre des combinaisons des premières caractéristiques: cercles, ... | Aux derniers niveaux, on différencie des éléments très complexes : voitures, motos, vélo, visage, ... En fait, ces derniers niveaux sont également une combinaison des niveaux précédents. Un visage c&#39;est un cercle avec des cercles pour les yeux, des lignes pour le nez, ... Interprétation des caractéristiques détectées par niveau | En analysant des images, on peut tout faire . Ce qui est marrant, c&#39;est que plein de gens ont réussi à exploiter des modèles d&#39;analyses d&#39;images pour détecter des virus informatiques ou bien analyser des sons ou autres. Et même mieux, ils ont obtenu des meilleurs résultats. . Si ca vous intéresse, aller regarder le cours de Jérémy Howard ! https://www.youtube.com/watch?v=BvHmRx14HQ8&amp;t=1706s .",
            "url": "https://jpbiomed.github.io/blogai/2021/06/20/AI-les-Bases.html",
            "relUrl": "/2021/06/20/AI-les-Bases.html",
            "date": " • Jun 20, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About me . . Hi! I am Jason Pettiaux. I am biomedical engineer based in Brussels, Belgium. . I am a very curious person and I am passionate about a lot of topics ranging from languages to knives throwing and from picking locks to philosophy and economical implications of a wide scale adoption of the free software principles in the global economy. . This is on top of my technological interests such as microfluidics, electrowetting, machine learning, … . My background . I was born in Brussels, halfway between the “Forêt de Soignes” and the ULB (Université Libre de Bruxelles) where the Fab Lab ULB is based. . Both my parents where lucky enough to go to university and our values were clear from the start: . Share what you have with those who don’t, starting with knowledge because giving it does not cost you at all. . | My father is a living embodiment of this mantra as he has been teaching for the biggest part of his carreer and spent his whole life fighting for the free software movement so we received RMS at home multiple times. . | . On my mother side, my grandfather had to hide during the war and thus the following mantra was taught to us early on:  . You own what you have in your head and what you can do with your hands, everything can be taken away. — Andre Sterling . After a comfortable childhood, I went to the local university, the ULB, where I studied Biomedical Engineering. During my studies, I did an Erasmus in Berlin and learned German. Berlin is great, I very much advise you to go there, to visit or to stay ! . Previous work . After my studies, I decided that I really wanted to learn another language so I moved to work in Barcelona and learn Spanish (no, no, not Catalan). There I met amazing people and discovered great projects and soon enough I wanted to build my own but what to build was still unclear. . After coming back to Belgium and a short entrepreneurial experience, I came back to my field, biomedical engineering and the medical device industry. . Now, I have been working in medical devices and IVD for almost 2 years and I loved it ! Working on products that save people’s life is really amazing ! . I now had tons of projects related to that field but I realized I did not have the full skill set to build them (for technical part at least), so I decided to join the FabAcademy! . I am building plenty of projects but they’re not technical for the most part so I can’t show nice pictures of them. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jpbiomed.github.io/blogai/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jpbiomed.github.io/blogai/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}